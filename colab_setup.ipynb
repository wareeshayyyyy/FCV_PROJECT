{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bone Fracture Detection - Google Colab Setup\n",
        "\n",
        "This notebook sets up and trains YOLO model for bone fracture detection with GPU support.\n",
        "\n",
        "## Features:\n",
        "- ‚úÖ Free GPU access (T4, 16GB VRAM)\n",
        "- ‚úÖ Complete training pipeline\n",
        "- ‚úÖ Automatic model download\n",
        "- ‚úÖ Training visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Dependencies installed!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install ultralytics opencv-python opencv-contrib-python scikit-image scipy -q\n",
        "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118 -q\n",
        "\n",
        "print(\"‚úÖ Dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "GPU INFORMATION\n",
            "============================================================\n",
            "CUDA Available: False\n",
            "‚ö†Ô∏è  No GPU! Go to: Runtime > Change runtime type > GPU\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Check GPU/CPU\n",
        "import torch\n",
        "print(\"=\"*60)\n",
        "print(\"DEVICE INFORMATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ CUDA Available: True\")\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"VRAM: {vram_gb:.2f} GB\")\n",
        "    print(\"‚úÖ GPU is ready for training!\")\n",
        "    recommended_model = 's' if vram_gb >= 8 else 'n'\n",
        "    print(f\"Recommended: YOLOv8{recommended_model}\")\n",
        "    device_type = \"GPU\"\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è  CUDA Available: False\")\n",
        "    print(\"‚úÖ CPU mode enabled - Training will work but will be slower\")\n",
        "    print(\"üí° Tip: For faster training, go to Runtime > Change runtime type > GPU (T4)\")\n",
        "    recommended_model = 'n'  # Use nano model for CPU\n",
        "    device_type = \"CPU\"\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Upload Dataset\n",
        "\n",
        "**Option A**: Upload from your computer (click folder icon on left)\n",
        "**Option B**: Mount Google Drive and copy dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive (if dataset is in Drive)\n",
        "# Note: This will only work in Google Colab, not in local Jupyter\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"‚úÖ Google Drive mounted!\")\n",
        "except ImportError:\n",
        "    print(\"‚ÑπÔ∏è  Running locally - Google Drive mount skipped\")\n",
        "    print(\"‚úÖ Ready for dataset upload (use local paths)\")\n",
        "\n",
        "# Or upload dataset.zip manually and unzip:\n",
        "# !unzip -q /content/archive.zip -d /content/bone_fracture_detection/data/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Training Configuration\n",
        "\n",
        "Modify these settings as needed:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Configuration\n",
        "DATA_YAML = \"/content/bone_fracture_detection/data/archive/bone fracture detection.v4-v4.yolov8/data.yaml\"\n",
        "\n",
        "# Auto-adjust based on device\n",
        "if torch.cuda.is_available():\n",
        "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    if vram_gb >= 16:\n",
        "        MODEL_SIZE = \"m\"  # medium\n",
        "        BATCH = 32\n",
        "    elif vram_gb >= 8:\n",
        "        MODEL_SIZE = \"s\"  # small\n",
        "        BATCH = 16\n",
        "    else:\n",
        "        MODEL_SIZE = \"n\"  # nano\n",
        "        BATCH = 8\n",
        "else:\n",
        "    MODEL_SIZE = \"n\"  # nano for CPU\n",
        "    BATCH = 4  # Smaller batch for CPU\n",
        "\n",
        "EPOCHS = 10       # 5-10 epochs\n",
        "IMGSZ = 640       # Image size\n",
        "\n",
        "print(f\"Configuration ({device_type}):\")\n",
        "print(f\"  Model: YOLOv8{MODEL_SIZE}\")\n",
        "print(f\"  Epochs: {EPOCHS}\")\n",
        "print(f\"  Batch Size: {BATCH} (adjusted for {device_type})\")\n",
        "print(f\"  Image Size: {IMGSZ}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Update Data YAML and Start Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "\n",
        "# Update data.yaml paths\n",
        "data_yaml_path = Path(DATA_YAML)\n",
        "if data_yaml_path.exists():\n",
        "    with open(data_yaml_path, 'r') as f:\n",
        "        data_config = yaml.safe_load(f)\n",
        "    \n",
        "    base_dir = data_yaml_path.parent\n",
        "    data_config['train'] = str(base_dir / 'train' / 'images')\n",
        "    data_config['val'] = str(base_dir / 'valid' / 'images')\n",
        "    data_config['test'] = str(base_dir / 'test' / 'images')\n",
        "    \n",
        "    updated_yaml = base_dir / 'data_updated.yaml'\n",
        "    with open(updated_yaml, 'w') as f:\n",
        "        yaml.dump(data_config, f)\n",
        "    \n",
        "    DATA_YAML = str(updated_yaml)\n",
        "    print(f\"‚úÖ Data YAML updated: {updated_yaml}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Dataset not found. Please upload dataset first!\")\n",
        "\n",
        "# Start Training\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*80)\n",
        "print(f\"This will take ~2-5 hours for {EPOCHS} epochs...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "model = YOLO(f\"yolov8{MODEL_SIZE}.pt\")\n",
        "\n",
        "results = model.train(\n",
        "    data=DATA_YAML,\n",
        "    epochs=EPOCHS,\n",
        "    imgsz=IMGSZ,\n",
        "    batch=BATCH,\n",
        "    device=0 if torch.cuda.is_available() else 'cpu',\n",
        "    project=\"/content/bone_fracture_detection/yolo_training_results\",\n",
        "    name=f\"yolov8{MODEL_SIZE}_bone_fracture\",\n",
        "    save=True,\n",
        "    plots=True,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Training Complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: DenseNet-121 Fine-Tuning\n",
        "\n",
        "Fine-tune DenseNet-121 for bone fracture classification with two-phase training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install additional dependencies for DenseNet training\n",
        "%pip install scikit-learn pandas matplotlib seaborn tqdm -q\n",
        "print(\"‚úÖ Additional dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 1: Clone from GitHub (Recommended)\n",
        "# Uncomment the line below to clone your project\n",
        "# !cd /content && git clone https://github.com/wareeshayyyyy/FCV_PROJECT.git bone_fracture_detection\n",
        "\n",
        "# Option 2: Setup manually\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "base_dir = Path('/content/bone_fracture_detection')\n",
        "if not (base_dir / 'run_complete_training.py').exists():\n",
        "    # Create directory structure if not cloned\n",
        "    base_dir.mkdir(exist_ok=True)\n",
        "    os.makedirs(base_dir / 'src/bonefracture', exist_ok=True)\n",
        "    os.makedirs(base_dir / 'checkpoints', exist_ok=True)\n",
        "    os.makedirs(base_dir / 'training_results', exist_ok=True)\n",
        "    \n",
        "    print(\"‚úÖ Directory structure created!\")\n",
        "    print(\"\\nüìÅ Next steps:\")\n",
        "    print(\"1. Upload your dataset to:\", base_dir / 'data/archive/')\n",
        "    print(\"2. Upload run_complete_training.py to:\", base_dir)\n",
        "    print(\"3. Upload src/ folder to:\", base_dir / 'src/')\n",
        "    print(\"\\nOr uncomment the git clone line above to clone from GitHub!\")\n",
        "else:\n",
        "    print(\"‚úÖ Project already cloned/uploaded!\")\n",
        "    print(f\"üìÅ Project directory: {base_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Run DenseNet Fine-Tuning\n",
        "\n",
        "This will run two-phase training:\n",
        "- **Phase 1**: Train classifier with frozen backbone (10 epochs)\n",
        "- **Phase 2**: Fine-tune all layers with differential LR (10 epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DenseNet Fine-Tuning Configuration\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "base_dir = Path('/content/bone_fracture_detection')\n",
        "sys.path.insert(0, str(base_dir))\n",
        "\n",
        "# Update dataset path for Colab (modify if your dataset is in a different location)\n",
        "DATASET_ROOT = base_dir / \"data/archive/bone fracture detection.v4-v4.yolov8\"\n",
        "\n",
        "# Update the path in run_complete_training.py if needed\n",
        "if (base_dir / 'run_complete_training.py').exists():\n",
        "    # Read and update the DATASET_ROOT in the script\n",
        "    with open(base_dir / 'run_complete_training.py', 'r') as f:\n",
        "        script_content = f.read()\n",
        "    \n",
        "    # Replace Windows path with Colab path\n",
        "    script_content = script_content.replace(\n",
        "        r\"DATASET_ROOT = r'data\\archive\\bone fracture detection.v4-v4.yolov8'\",\n",
        "        f\"DATASET_ROOT = r'{DATASET_ROOT}'\"\n",
        "    )\n",
        "    \n",
        "    with open(base_dir / 'run_complete_training.py', 'w') as f:\n",
        "        f.write(script_content)\n",
        "    \n",
        "    print(\"‚úÖ Updated dataset path in run_complete_training.py\")\n",
        "\n",
        "# Auto-adjust batch size for DenseNet based on device\n",
        "if torch.cuda.is_available():\n",
        "    densenet_batch = 16\n",
        "else:\n",
        "    densenet_batch = 4  # Smaller batch for CPU\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"DENSENET-121 FINE-TUNING CONFIGURATION\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Dataset: {DATASET_ROOT}\")\n",
        "print(f\"Dataset exists: {Path(DATASET_ROOT).exists()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Device: GPU - {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(f\"Device: CPU (Training will be slower but will work)\")\n",
        "print(f\"Batch Size: {densenet_batch} (auto-adjusted for device)\")\n",
        "print(f\"Phase 1 Epochs: 10 (Classifier training)\")\n",
        "print(f\"Phase 2 Epochs: 10 (Full fine-tuning)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Update batch size in the training script\n",
        "if (base_dir / 'run_complete_training.py').exists():\n",
        "    with open(base_dir / 'run_complete_training.py', 'r') as f:\n",
        "        script_content = f.read()\n",
        "    \n",
        "    # Update batch size\n",
        "    script_content = script_content.replace(\n",
        "        \"BATCH_SIZE = 16\",\n",
        "        f\"BATCH_SIZE = {densenet_batch}\"\n",
        "    )\n",
        "    \n",
        "    with open(base_dir / 'run_complete_training.py', 'w') as f:\n",
        "        f.write(script_content)\n",
        "    \n",
        "    print(f\"‚úÖ Updated batch size to {densenet_batch} in run_complete_training.py\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run fine-tuning\n",
        "# Make sure run_complete_training.py is uploaded and dataset path is correct\n",
        "\n",
        "import subprocess\n",
        "import os\n",
        "import sys\n",
        "\n",
        "os.chdir('/content/bone_fracture_detection')\n",
        "sys.path.insert(0, '/content/bone_fracture_detection')\n",
        "\n",
        "# Check if file exists\n",
        "if os.path.exists('run_complete_training.py'):\n",
        "    print(\"‚úÖ Starting DenseNet fine-tuning...\")\n",
        "    print(\"This will take ~1-2 hours for 20 epochs total...\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Update dataset path in the script if needed\n",
        "    # The script should use: DATASET_ROOT = '/content/bone_fracture_detection/data/archive/bone fracture detection.v4-v4.yolov8'\n",
        "    \n",
        "    # Run training (use exec to see real-time output)\n",
        "    exec(open('run_complete_training.py').read())\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  run_complete_training.py not found!\")\n",
        "    print(\"Please upload the file to /content/bone_fracture_detection/\")\n",
        "    print(\"\\nOr clone from GitHub:\")\n",
        "    print(\"!cd /content && git clone https://github.com/wareeshayyyyy/FCV_PROJECT.git bone_fracture_detection\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display training results and download models\n",
        "from IPython.display import Image, display\n",
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "\n",
        "results_dir = Path('/content/bone_fracture_detection/training_results')\n",
        "checkpoints_dir = Path('/content/bone_fracture_detection/checkpoints')\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"TRAINING RESULTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Display training history plot\n",
        "history_plot = results_dir / 'complete_training_history.png'\n",
        "if history_plot.exists():\n",
        "    display(Image(str(history_plot)))\n",
        "    print(\"‚úÖ Training history plot displayed!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Training history plot not found yet.\")\n",
        "\n",
        "# Download best models\n",
        "models_to_download = [\n",
        "    checkpoints_dir / 'best_model_phase_1.pth',\n",
        "    checkpoints_dir / 'best_model_phase_2.pth',\n",
        "    checkpoints_dir / 'final_model_complete.pth'\n",
        "]\n",
        "\n",
        "print(\"\\nüì• Downloading models...\")\n",
        "for model_path in models_to_download:\n",
        "    if model_path.exists():\n",
        "        files.download(str(model_path))\n",
        "        print(f\"‚úÖ Downloaded: {model_path.name}\")\n",
        "        \n",
        "        # Also save to Drive (if mounted)\n",
        "        try:\n",
        "            from google.colab import drive\n",
        "            import shutil\n",
        "            drive_path = Path('/content/drive/MyDrive') / model_path.name\n",
        "            if Path('/content/drive/MyDrive').exists():\n",
        "                shutil.copy(model_path, drive_path)\n",
        "                print(f\"‚úÖ Saved to Drive: {model_path.name}\")\n",
        "            else:\n",
        "                print(f\"‚ÑπÔ∏è  Google Drive not mounted - skipping Drive save\")\n",
        "        except (ImportError, Exception):\n",
        "            print(f\"‚ÑπÔ∏è  Google Drive not available - skipping Drive save\")\n",
        "\n",
        "# Download training results JSON\n",
        "results_json = results_dir / 'complete_training_results.json'\n",
        "if results_json.exists():\n",
        "    files.download(str(results_json))\n",
        "    print(\"‚úÖ Training results JSON downloaded!\")\n",
        "    \n",
        "    # Display results summary\n",
        "    import json\n",
        "    with open(results_json, 'r') as f:\n",
        "        results = json.load(f)\n",
        "    print(\"\\nüìä Final Results Summary:\")\n",
        "    print(f\"  Phase 1 Best Val Accuracy: {results.get('phase1_best_val_acc', 'N/A'):.4f}\")\n",
        "    print(f\"  Phase 2 Best Val Accuracy: {results.get('phase2_best_val_acc', 'N/A'):.4f}\")\n",
        "    print(f\"  Final Test Accuracy: {results.get('final_test_acc', 'N/A'):.4f}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Training results JSON not found yet.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display training results\n",
        "from IPython.display import Image, display\n",
        "\n",
        "results_dir = f\"/content/bone_fracture_detection/yolo_training_results/yolov8{MODEL_SIZE}_bone_fracture\"\n",
        "best_model = f\"{results_dir}/weights/best.pt\"\n",
        "\n",
        "try:\n",
        "    display(Image(f\"{results_dir}/results.png\"))\n",
        "    display(Image(f\"{results_dir}/confusion_matrix.png\"))\n",
        "    print(f\"‚úÖ Best model: {best_model}\")\n",
        "except Exception as e:\n",
        "    print(f\"Results available in: {results_dir}\")\n",
        "\n",
        "# Download model\n",
        "from google.colab import files\n",
        "if Path(best_model).exists():\n",
        "    files.download(best_model)\n",
        "    print(\"‚úÖ Model downloaded!\")\n",
        "    \n",
        "    # Also save to Drive\n",
        "    !cp {best_model} /content/drive/MyDrive/\n",
        "    print(\"‚úÖ Model saved to Google Drive!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
